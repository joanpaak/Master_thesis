%!Rnw root = ../Joni_Paakko_-_Thesis.Rnw

\section{Simulations}
\label{sec:simulations}

In this context simulation refers to using simulated observers as participants. A set of parameter values--$\sigma$, $\beta$, $\kappa_{\mu}$ and so on--are chosen for the simulated observer, and they are used to generate simulated responses. This results in simulated data sets that otherwise function similarly to datasets generated by human observers, but since the generating parameters are chosen by the experimenter the ground truth is known. 

Evaluating models by simulating data is used in Bayesian statistics in general \citep[Chapter 10]{bda} and in psychophysics for studying the performance of the models (e.g. in the presence of lapsing trials in \citet{prins2012}) or adaptive algorithms (e.g. by \citet{kontsevichtyler1999}). \citet{oron2007} has criticized simulation studies as a substitute for thorough analytical solutions: the results of simulations--as I mentioned in the Introduction--are contingent on several \textit{ad hoc} assumptions, and as such it is not always clear if the results are generalizable beyond the initial study. However this can also be seen as the strength of simulation studies: it's possible to easily test models and assumptions that escape analytical solutions.

I am using simulations to answer  two main questions: 

\begin{enumerate}
  \item How much more efficient the adaptive algorithm is in relation to sampling stimuli from a fixed grid--if at all? 
  \item How well can generating parameters be recovered?
\end{enumerate}

These questions are closely related, since relative efficiency of the algorithms (Question 1) is defined here by the quantities that are also used to evaluate Question 2. 

There are two quantities of interest. First is defined by taking the means of the marginal posterior distributions as point estimates and calculating the squared differences to the generating parameters. This quantifies squared bias and variance of these estimators. The second quantity is the standard deviations of the marginal posterior distributions. This is used as a measure of how much uncertainty about the parameters is left after the data collection process. The goal, as already stated, is to minimize uncertainty about the parameters.

Question 1 is evaluated by inspecting if there are differences between the algorithms in how quickly the aforementioned quantities approach zero. Question 2 is answered by looking at the same  quantities, but the focus is on the overall performance, not differences. The first question is more closely related to the topic of this thesis, but the second question has more general value regarding the estimation of GRT models. 

Also, the simulations were intended as providing preliminary information for the psychophysical experiment (Section \ref{sec:pp_exp} \textit{\nameref{sec:pp_exp}}) in which participants' task was to detect pitch and timbre differences. Thus although not mentioned beyond the choice of priors, the dimensions of stimuli in the simulation are implicitly pitch and timbre.

During the simulations the model with $\kappa_{\mu}$ (which will be labelled \textit{Model 1} in Section \ref{sec:pp_exp} \textit{\nameref{sec:pp_exp}}) was used to determine the stimulus choices for two reasons. First, it's the most widely used model in the GRT related literature (or, the classic model that is analogous to it, see Section \ref{sec:grt_criticism} \textit{\nameref{sec:grt_criticism}}: The models used e.g. in the 2x2 categorization experiments commonly model interactions as interaction between means of the distributions. Second, there are some results suggesting timbre affects the pitch of signal \citep{allen2014, platt1985}, a phenomenon that would most naturally be modelled as shift in the mean of the evidence distribution; on the other hand, according to \cite{silbert2009} the main source of interaction between pitch and timbre arises from correlated noise, which again is included in the model as the parameter $\rho$.

\subsection{Methods}

The general method was the following: first a set of generating parameters for the simulated observer were drawn randomly, and then either of the algorithms (adaptive/non-adaptive) described earlier were run. This was done for both the Yes/No and 21-4AFC procedure, resulting in four different conditions. I have chosen the number 800 fairly arbitrarily; it represents a number of trials that, I think, could still be administered relatively continuously to a participant, without taking into account non-stationarities induced by a multi-session design\footnote{This was the number of trials completed by both participants during one session of the psychophysical experiment conducted for this thesis, see Section \ref{sec:pp_exp} \textit{\nameref{sec:pp_exp}}}. 

\paragraph{Prior distributions}

Prior distributions and distributions from which generating parameters for the simulations were drawn from are shown in Figure \ref{fig:priors} and in tabular form in Table \ref{tab:priors}. The same prior and generating distribution is used for both dimensions, that is the prior labelled $\sigma$ is used for both of the $\sigma$ parameters in the model. Note that the scale for criterion is given in false alarm probabilities for easier interpretation. 

Priors for the parameters were chosen based on prior information from \citet{silbert2009, dai2011} and from pilot testing. Priors for the $\sigma$ parameters were chosen to on average reflect the expected sensitivity of the observers in this kind of task, with two caveats: 1) timbre changes were assumed to be divided by ten, that is a timbre change of 2 would correspond to a change of 20Hz on the original scale (see Section \ref{sec:pp_exp} \textit{\nameref{sec:pp_exp}} for more information about how the stimuli were defined) and 2) standard deviation for the prior was large to reflect the possibility of widely differing thresholds.

Priors for the interactions were centred on zero, acting as a way to keep inferences conservative.

Generating parameters for the simulations were drawn from bimodal distributions. The idea was to draw values that are covered by the prior, but which do not necessarily correspond with the mode of the prior distribution. Another motivation was to have qualitatively different simulated observers: some that have high values for some of the parameters and others that have low values.

\begin{figure}[!htb]
\centering
<<echo=F>>=

prior = read.table("../../Simulation_and_experiment/Priors.dat", header = T)

p = sapply(as.matrix(prior), function(x) {eval(expr = parse(text = toString(x)))})
p = matrix(as.vector(p), ncol = 5, nrow = 5)

par(mfrow=c(3,2))
par(family = font_family_global)

# Prior for alpha

x = seq(0, 6, length.out = 200) 
y_prior = dlnorm(x, p[1,1], p[1,2])
y_simu = dlnorm(x, p[1,3], p[1,5]) + dlnorm(x, p[1,4], p[1,5])

y_prior = y_prior / max(y_prior)
y_simu = y_simu / max(y_simu)

plot(x, y_prior, type = "l", ylab = "Density", xlab = expression(sigma), 
     lwd = 2, cex.lab = 1.25, cex.axis = 1.25)
polygon(x, y_simu, col = highlight_col, border = rgb(0, 0, 0, 0))

# points(x, y_simu, type = "l", lty = 2, lwd = 2)
abline(h = seq(0, 1, 0.2), lty = 3, col = rgb(0, 0, 0, 0.4))

# Prior for criterion

x = seq(0.5, 2.5, length.out = 200) 
x_p = pnorm(-x)

y_prior = dnorm(x, p[2,1], p[2,2])
y_simu = dnorm(x, p[2,3], p[2,5]) + dnorm(x, p[2,4], p[2,5])

y_prior = y_prior / max(y_prior)
y_simu = y_simu / max(y_simu)

plot(x, y_prior, type = "l", ylab = "Density", xlab = "False alarm probability", 
     lwd = 2, cex.lab = 1.25, cex.axis = 1.25, xaxt = "n")
polygon(x, y_simu, col = highlight_col, border = rgb(0, 0, 0, 0))

#points(x, y_simu, type = "l", lty = 2, lwd = 2)
axis(1, at = x[seq(1, length(x), length.out= 5)], 
     labels = round(x_p[seq(1, length(x), length.out= 5)], 2), cex.axis = 1.25)
abline(h = seq(0, 1, 0.2), lty = 3, col = rgb(0, 0, 0, 0.4))


# Prior for beta

x = seq(0, 2, length.out = 200) 
y_prior = dlnorm(x, p[3,1], p[3,2])
y_simu = dlnorm(x, p[3,3], p[3,5]) + dlnorm(x, p[3,4], p[3,5])

y_prior = y_prior / max(y_prior)
y_simu = y_simu / max(y_simu)

plot(x, y_prior, type = "l", ylab = "Density", xlab = expression(beta), 
     lwd = 2, cex.lab = 1.25, cex.axis = 1.25)
polygon(x, y_simu, col = highlight_col, border = rgb(0, 0, 0, 0))

# points(x, y_simu, type = "l", lty = 2, lwd = 2)
abline(h = seq(0, 1, 0.2), lty = 3, col = rgb(0, 0, 0, 0.4))

# Kappa mu

x = seq(-0.85, 0.85, length.out = 200) 
y_prior = dnorm(x, p[4,1],p[4,2])
y_simu = dnorm(x, p[4,3],p[4,5]) + dnorm(x, p[4,4], p[4,5])

y_prior = y_prior / max(y_prior)
y_simu = y_simu / max(y_simu)

plot(x, y_prior, type = "l", ylab = "Density", xlab = expression(kappa), 
     lwd = 2, cex.lab = 1.25, cex.axis = 1.25)
polygon(x, y_simu, col = highlight_col, border = rgb(0, 0, 0, 0))

#points(x, y_simu, type = "l", lty = 3, lwd = 2)
abline(h = seq(0, 1, 0.2), lty = 3, col = rgb(0, 0, 0, 0.4))

# Rho

x = seq(-1, 1, length.out = 200) 
x_rho = atanh(x)
y_prior = dnorm(x_rho, p[5,1], p[5,2])
y_simu = dnorm(x_rho, p[5,3], p[5,5]) + dnorm(x_rho, p[5,4], p[5,5])

y_prior = y_prior / max(y_prior)
y_simu = y_simu / max(y_simu)

plot(x, y_prior, type = "l", ylab = "Density", xlab = expression(rho), 
     lwd = 2, cex.lab = 1.25, cex.axis = 1.25, xaxt = "n")
axis(1, at = x[seq(1, length(x), length.out = 5)], 
     labels = round(x[seq(1, length(x), length.out = 5)], 2), cex.axis = 1.25)
polygon(x, y_simu, col = highlight_col, border = rgb(0, 0, 0, 0))

#points(x, y_simu, type = "l", lty = 3, lwd = 2)
abline(h = seq(0, 1, 0.2), lty = 3, col = rgb(0, 0, 0, 0.4))
@

\caption{Prior distributions for the parameters of the model (solid black lines) and distributions for generating parameters for the simulations (regions shaded with red). Note that all of the densities are normalized to have maximum value of 1.0.}
\label{fig:priors}
\end{figure}

\begin{table}[H]
\centering
\caption{Parameters used for prior distributions and distributions of generating parameters. $M^l$ and $M^u$, respectively, are for the lower and upper peaks of bimodal distributions.}
\vspace{0.5cm}
\begin{tabular}{cccccc}
\toprule

          & \multicolumn{2}{c}{Prior} & \multicolumn{3}{c}{Generating}   \\
          \cmidrule(lr){2-3}\cmidrule(lr){4-6}
          & $M$       & $SD$    & $M^l$         & $M^u$         & $SD$   \\
\midrule
$\sigma$  & $log(\Sexpr{exp(p[1,1])})$  & $\Sexpr{p[1,2]}$   & $log(\Sexpr{exp(p[1,3])})$    & $log(\Sexpr{exp(p[1,4])})$    & $\Sexpr{p[1,5]}$ \\
$C$       & $\Sexpr{p[2,1]}$            & $\Sexpr{p[2,2]}$   & $\Sexpr{p[2,3]}$         & $\Sexpr{p[2,4]}$         & $\Sexpr{p[2,5]}$  \\
$\beta$   & $log(\Sexpr{exp(p[3,1])})$  & $\Sexpr{p[3,2]}$   & $log(\Sexpr{exp(p[3,3])})$    & $log(\Sexpr{exp(p[3,4])})$    & $\Sexpr{p[3,5]}$ \\
$\kappa$  & $\Sexpr{p[4,1]}$            & $\Sexpr{p[4,2]}$   & $\Sexpr{p[4,3]}$        & $\Sexpr{p[4,4]}$         & $\Sexpr{p[4,5]}$ \\
$\rho$    & $\Sexpr{p[5,1]}$            & $\Sexpr{p[5,2]}$  & $atanh(\Sexpr{tanh(p[5,3])})$ & $atanh(\Sexpr{tanh(p[5,4])})$  & $\Sexpr{p[5,5]}$ \\
\bottomrule
\end{tabular}
\label{tab:priors}
\end{table}

\subsection{Results}

Total number of simulations per condition are shown in Table \ref{tab:conditions}. Squared errors and marginal standard deviations are presented in two ways: 1) on trial-by-trial basis and 2) by estimating the average differences on the last trial.

\begin{table}[H]
\centering
\caption{Conditions and number of simulations in each.}
\vspace{0.5cm}
\begin{tabular}{ccc}

\toprule
Procedure & Algorithm & N \\
\midrule
Yes/No & Adaptive & 184 \\
Yes/No & Random & 284 \\
2I-4AFC & Adaptive & 174 \\
2I-4AFC & Random & 130 \\

\bottomrule

\end{tabular}

\label{tab:conditions}
\end{table}

\paragraph{Trial-by-trial estimates}
Trial-by-trial results from the simulations are summarized in Figures \ref{fig:simulation_YN_sensory_sq_error} to \ref{fig:simulation_AFC_interaction_SD}. The figures show squared errors in relation to generating parameters and marginal standard deviations after $N$ trials, all the way to trial number 800. In all plots black color is used for the randomly sampled stimuli while red is used for adaptively sampled stimuli. The shaded regions indicate 50\%-quantiles (from 25\% to 75\%); solid lines indicate medians. Sensory ($\sigma$, $\beta$, crit) and interaction ($\kappa_{\mu}$, $\rho$) are shown in their own figures.

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_YN_sensory_sq_error}
\caption{Procedure: Yes/No; sensory parameters. Trial-by-trial squared error between marginal means of the posterior distribution and generating parameters. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_YN_sensory_sq_error}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_YN_interaction_sq_error}
\caption{Procedure: Yes/No; interaction parameters. Trial-by-trial squared error between marginal means of the posterior distribution and generating parameters. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_YN_interaction_sq_error}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_YN_sensory_SD}
\caption{Procedure: Yes/No; sensory parameters. Trial-by-trial marginal standard deviations. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_YN_sensory_SD}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_YN_interaction_SD}
\caption{Procedure: Yes/No; interaction parameters. Trial-by-trial marginal standard deviations. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_YN_interaction_SD}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_AFC_sensory_sq_error}
\caption{Procedure: 2I-4AFC; sensory parameters. Trial-by-trial squared error between marginal means of the posterior distribution and generating parameters. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_AFC_sensory_sq_error}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_AFC_interaction_sq_error}
\caption{Procedure: 2I-4AFC; interaction parameters. Trial-by-trial squared error between marginal means of the posterior distribution and generating parameters. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_AFC_interaction_sq_error}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_AFC_sensory_SD}
\caption{Procedure: 2I-4AFC; sensory parameters. Trial-by-trial marginal standard deviations. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_AFC_sensory_SD}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75, angle = 0]{simulation_AFC_interaction_SD}
\caption{Procedure: 2I-4AFC; interaction parameters. Trial-by-trial marginal standard deviations. Red: adaptive algorithm; black: random stimuli.}
\label{fig:simulation_AFC_interaction_SD}
\end{figure}

\paragraph{Hierarchical model}

A hierarchical model was fit to both the marginal standard deviations and squared errors at the last trial to get a more quantitative understanding of the differences at that point. This should be considered as a first-order approximation of a more complete model of performance. Why only the estimates at the last trial are presented is because for any simple model which breaks the performance down in two factors, namely the final values and the speed at which they were arrived at, only one of the factors needs to be known. This is because all of the algorithms start from the same values (the prior distribution) and are run for the same amount of time (800 trials). Of course some more sophisticated model, that would perform more complex decomposition of performance could be used, but the development of such algorithm is beyond the scope of this thesis. 

A common dummy coded linear model with Gaussian errors was used. The slopes, intercepts and standard deviations were pooled inside each condition, which means that e.g. all of the marginal standard deviations in the Yes/No condition with adaptively selected stimuli were given a common hyperprior. The models were fit using Stan.

Data was standardized before fitting ($y' = (y - \overline{y}) / \text{SD}(y) $). Parameter-specific $\beta'$ coefficients (slopes fit to standardized data) of the linear model are shown in Figures from \ref{fig:qs_YN_SD} to \ref{fig:qs_AFC_sq_error}. In all plots the thicker parts indicate 50\% and narrower lines 95\% equal-tailed intervals\footnote{\textit{Equal-tailed interval} (ETI) means the interval between some percentiles of the posterior distribution \citep[p. 342]{kruschke2015}, here for example the interval between 2.5\% and 97.5\% for the 95\% ETI.}. Positive values indicate better performance by the adaptive algorithm.

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{qs_YN_SD}
\caption{Procedure: Yes/No. $\beta'$ coefficients for the difference between adaptive and random algorithms in marginal standard deviations.}
\label{fig:qs_YN_SD}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{qs_YN_sq_error}
\caption{Procedure: Yes/No. $\beta'$ coefficients for the difference between adaptive and random algorithms in squared errors between marginal means and generating parameters.}
\label{fig:qs_YN_sq_error}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{qs_AFC_SD}
\caption{Procedure: 2I-4AFC. $\beta'$ coefficients for the difference between adaptive and random algorithms in marginal standard deviations.}
\label{fig:qs_AFC_SD}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{qs_AFC_sq_error}
\caption{Procedure: 2I-4AFC. $\beta'$ coefficients for the difference between adaptive and random algorithms in squared errors between marginal means and generating parameters.}
\label{fig:qs_AFC_sq_error}
\end{figure} 

\subsection{Discussion}

\paragraph{Question 1: is the adaptive algorithm more efficient?}

Judging from Figures \ref{fig:qs_YN_SD} and \ref{fig:qs_AFC_SD}, by the time of 800 completed trials the adaptive algorithm has managed to reduce marginal standard deviations more for parameters $\sigma$, $\beta$ and $\rho$ in both conditions. Effect sizes range from around $0.4$ to almost $1.0$. However, as can be seen from Figures \ref{fig:simulation_YN_sensory_SD}, \ref{fig:simulation_YN_interaction_SD}, \ref{fig:simulation_AFC_sensory_SD} and \ref{fig:simulation_AFC_interaction_SD}, differences in raw scores are fairly modest, around 0.05 to 0.20.

There doesn't seem to be that much difference in $\kappa_{\mu}$ parameters, which is probably explained by the fact that, as can be seen from Figures \ref{fig:simulation_AFC_interaction_SD} and \ref{fig:simulation_YN_interaction_SD} the marginal standard deviations for these parameters reduce fairly quickly.

The most surprising result is that random sampling seems to be more effective in reducing uncertainty about the criteria (Figure \ref{fig:qs_YN_SD}). 

Similar pattern can be observed for the squared errors (Figures \ref{fig:qs_YN_sq_error} and \ref{fig:qs_AFC_sq_error}) but the differences are swamped by a lot more variability. 

\paragraph{Question 2: how well are generating parameters recovered?}

It seems that the majority of of improvement for most parameters happens before 400 trials. After that, information gain seems to slow down considerably.

The most problematic parameters would seem to be the $\beta$ parameters. This contrasts results in for example \cite{kontsevichtyler1999}, and could indicate that when in this more complex model inferences about the non-linearity of the $d'$ function become more uncertain. Note also that the variance of squared error in estimating the $\beta$ parameter increses somewhat during the first 100 trials (Figures \ref{fig:simulation_YN_sensory_sq_error} and \ref{fig:simulation_AFC_sensory_sq_error}, but the effect is more pronounced in the Yes/No condition), implying that for some simulations the posterior means drift further from the generating parameters. 

With regard to the interaction terms, squared errors and marginal standard deviations for the $\kappa_{\mu}$ parameters seem to approach zero fairly quickly, most of the improvements seems to have happened well before 100 trials, but the $\rho$ parameters seem more problematic (Figures \ref{fig:simulation_YN_interaction_sq_error}, \ref{fig:simulation_YN_interaction_SD}, \ref{fig:simulation_AFC_interaction_sq_error} and \ref{fig:simulation_AFC_interaction_SD}). Marginal standard deviations for the $\rho$s don't get, on average, much smaller than $.2$ which still implies a lot of uncertainty--keeping in mind that correlation coefficients are bound between $-1$ and $1$.
