%Rnw root = "../Joni_Paakko_-_Thesis.Rnw"

\section{Adaptive estimation}
\label{sec:adaptive}

The motivation behind adaptive psychophysical testing can be traced to intuitive enough starting point. Suppose that the researcher is interested in finding out what is the faintest possible sound that the subject is able to detect. It wouldn't make sense to waste time presenting them with stimuli that they can always detect, rather, if the subject has very low threshold for detection, it would make sense to reduce the level of the stimuli until the subject starts making errors, which is how adaptive \textit{staircase methods work} (c.f. \citet[Chapter 5]{kingdomprins2010}).

A popular subclass of adaptive methods are based on Bayesian statistics. \cite{watson1983} introduced the \textit{QUEST} procedure which was influential in its use of Bayesian statistics in determining which stimuli to choose. The QUEST procedure was used only for estimating one parameter, threshold (analogous to the parameter $\sigma$ in this thesis),  and it was later expanded by \citet{king_smith1997} in their ZEST algorithm for estimating the threshold and slope of the psychometric function (analogous to the parameters $\sigma$ and $\beta$ in this thesis). 

However, it wasn't until the work by \citet{kontsevichtyler1999} that Bayesian Adaptive Estimation as how it would be known today would emerge. The aforementioned QUEST and ZEST procedures used \textit{ad hoc} heuristics for determining the placement of the stimuli: in the QUEST procedure stimuli are placed near the threshold, in the ZEST procedure, in order to estimate the slope of the psychometric function, stimuli are placed above and below the threshold. A symptom of such \textit{ad hoc} methods is that \citet{treutwein1995} lists 22 adaptive procedures in his Table 1--and this does not include procedures for two-dimensional stimuli.

In \citet{kontsevichtyler1999} the stimulus selection heuristic is entirely based on principles of Bayesian statistics, and by that virtue entirely abstract and thus generalizable--at least in theory, no \textit{ad hoc} rules for choosing stimuli are needed. Indeed, this principle has been applied with success to different tasks and models in psychophysics, and also to estimation of multidimensional psychometric functions. 

The approach I use differs from previous work on adaptive methods with multidimensional signals. In the approaches by \citet{dimattina2015}, \citet{lesmes2006}, \citet{shen2013, shen2014} and \citet{kujalalukka2006} Bernoulli distributed responses are modelled, that is, only two response categories are used. In the QUEST+ procedure introduced by \citet{watson2017} it is possible to use multinomial distribution as a model of the responses, but the approach is quite different, e.g. the correlation is not modelled and the approach is not based on GRT.

The algorithm in \citet{kontsevichtyler1999} chooses stimuli that minimize the \textit{expected entropy} of the posterior distribution. In simple terms, entropy refers to the evenness of a probability distribution \citep[p. 365]{kruschke2015}. This is demonstrated in Figure \ref{fig:entropy}. The figure depicts two multinomial distributions, in which the categories refer to the possible responses for two-dimensional stimuli (see Section \ref{sec:grt_mdls} \textit{\nameref{sec:grt_mdls}}). The distribution on the left is maximally even, while the distribution on the right side has more probability assigned to the responses (0,0) and (1,1). Consequently, the distribution on the right has less entropy.

\begin{figure}
\begin{center}
<<echo=F, fig=T, fig.height=4>>=
cats = c("(0,0)", "(1,0)", "(0,1)", "(1,1)")
dist_1 = c(0.25, 0.25, 0.25, 0.25)
dist_2 = c(0.65, 0.05, 0.05, 0.25)

par(family = "Palatino")
par(mfrow = c(1, 2))
barplot(dist_1, names.arg = cats, ylim = c(0, 1), col = rgb(0.8, 0.3, 0.2, 0.7), border = NA)
mtext("Response", side = 1, line = 2)
mtext("Probability", side = 2, line = 2)

legend("topright", legend = 
         paste("Entropy = ", round(-sum(dist_1 * log(dist_1)),2)),
         bty = "n")
barplot(dist_2, names.arg = cats, ylim = c(0, 1), col = rgb(0.8, 0.3, 0.2, 0.7), border = NA)
mtext("Response", side = 1, line = 2)
mtext("Probability", side = 2, line = 2)

legend("topright", legend = 
         paste("Entropy = ", round(-sum(dist_2 * log(dist_2)),2)),
       bty = "n")

@
\end{center}
\caption{Two multinomial distributions that represent the probabilities for the possible response categories in the model. The distribution on the right is less even, and as a consequence has less entropy.}
\label{fig:entropy}
\end{figure}

In terms of posterior distribution this means that stimuli, that make the posterior probability densities as concentrated around a single point as possible are chosen. Recall from the previous section on Bayesian statistics that the spread of the posterior distribution for the parameters quantifies uncertainty related to the values of the parameters. Thus, minimizing the spread of the posterior entropy minimizes uncertainty about their values. 

The reason for using the distribution of responses in the above example is that \citet{kujalalukka2006} and \citet{kujala2011} show that minimizing the entropy of the response distribution is equivalent to minimizing the entropy of the posterior distribution of the parameters. This is, in my opinion, simpler than minimizing the entropy of the posterior distribution directly as done by \citet{kontsevichtyler1999}. 

The intuition behind this is that the response probabilities for a given stimulus are calculated from distributions of parameter values. Instead of continuous distributions, it is easier to think randomly sampled vectors of parameters as was shown in Table \ref{table:particleSet} when posterior approximation methods were discussed in Section \ref{sec:bayes} \textit{\nameref{sec:bayes}}. If there's only one parameter vector with non-zero weight (indicating low entropy in the posterior distribution), there's only one possible response; if there's a wide range of possible parameter vectors (indicating high entropy in the posterior distribution), these will also result in wide range of possible responses. 

\citet{kujalalukka2006} and \citet{kujala2011} give equations for calculating information gain using a set of IID particles--that is, particles with uniform weights. Since a weighted set is used by the algorithm I use (see Section \ref{sec:bayes} \textit{\nameref{sec:bayes}}), the equations are modified to accommodate this:

\begin{equation}
h(\sum_{i=1}^N w_i \psi_2(\bm{S};\theta_i)) - \sum_{i=1}^N h(w_i \psi_2(\bm{S};\theta_i)) 
\label{eq:infgain}
\end{equation}

in which $h$ is the entropy of a discrete distribution, in this case, a distribution the response probabilities. Each $\theta_i$ is a "slice" of the matrix of particles (a single  row of Table \ref{table:particleSet}), defining a single set of parameter values. $h$ is \citep{kontsevichtyler1999}:

\begin{equation}
h(p) = -\sum_{i = 1}^{N} p_i ln p_i
\end{equation}

On each trial the stimulus that maximizes Equation \ref{eq:infgain} is chosen.  

\paragraph{Some practical considerations}
\label{sec:practical_considerations}

In the preceding discussion I described the adaptive algorithm theoretically. In implementing the algorithm in R \citep{r_language}, some practical issues came up. 

First, a look-up table of the bivariate integrals (the function $\phi_2$) was pre-computed to speed up the calculations. 

Second, maximizing the information gain function using optimization algorithms proved prohibitively slow and unreliable. For this reason the possible stimuli were chosen from a grid of 15 stimuli. This allowed me to pre-compute response probabilities for all of the stimuli, which sped up the algorithm considerably. 

Third, during the adaptive run, whenever the particle set was rejuvenated, the ranges for the grid, from which the stimuli were chosen were re-computed. The lower and upper limits for stimuli were found from inverting the $d'$ function, and then finding stimulus levels that would correspond to $d' = 0.1$ for the lower limit and $d' = 2$ for the upper limit (Equation \ref{eq:limits}). Posterior means for the $\sigma$ and $\beta$ parameters were used, in addition, to take into account posterior uncertainty, these were shifted by 1.96 standard deviations to produce psychometric functions with either steep slope (high $\beta$) and low threshold (low $\sigma$) or shallow slope and high threshold. These corresponded to the most extreme scenarios: if the observer's psychometric function has a shallow slope and high threshold, response probabilities change relatively slowly and the range of stimuli has to be larger; the inverse is true if the threshold is slow and the slope steep. 

\begin{align}
\begin{split}
\label{eq:limits}
S_{\text{lower}} &= 0.1^{(1.0 / exp(E[\beta] + 1.96 * SD[\beta])) exp(E[\sigma] - 1.96 * SD[\sigma])}
\\ 
S_{\text{higher}} &= 2.0^ {(1.0 / exp(E[\beta] - 1.96 * SD[\beta])) exp(E[\sigma] + 1.96 * SD[\sigma])}
\end{split}
\end{align}

Fourth, to ensure that the particle approximation is accurate, three parallel particle algorithms were run. If the algorithms diverged--here defined as the marginal means differing more that $.2$ on the prior scale--Laplace approximation was used to start the particle sets again.