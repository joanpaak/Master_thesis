%!Rnw root = ../Joni_Paakko_-_Thesis.Rnw

\subsection{General Recognition Theory}
\label{sec:grt_mdls}

So far I have covered the case in which the participant has to make a single decision, e.g. if the pitch of the stimulus changed. In the two-dimensional case the participant has to observe two things at once. For example did the pitch change \textit{and} did the timbre change. The question of primary interest is if there are any \textit{interactions} between the dimensions: does changing timbre also change response probabilities to pitch changes.

A widely used theoretical framework for generalizing SDT into multiple dimensions is the General Recognition Theory. As in SDT, also in GRT randomness in the responses is thought to arise from the evidence having a probabilistic nature \citep{ashby1986, ashby2015, kadlec1992}. 

In the two-dimensional case also the evidence distribution is two dimensional. Following the assumption made earlier that evidence is distributed normally, it is possible to represent evidence in two dimensions by a bivariate normal distribution. A bivariate normal distribution can be defined by two means, which correspond to its location on each dimension, and a correlation coefficient, which corresponds to the shape of the distribution. A bivariate normal distribution with positive correlation is shown graphically in Figure \ref{fig:2dimnorm}. Left panel shows a projection of the 3-dimensional shape of the bivariate density; right panel shows the same distribution from above, as a contour. Rest of the figures in this thesis will represent bivariate distributions as contours similar to the one in the right panel.

\begin{figure}
\centering
<<echo=F,fig=T,fig.height = 3.5>>=
library(mvtnorm)
y = seq(-2, 4, length.out = 50)
x = seq(-2, 4, length.out = 50)
expected = matrix(NaN, nrow = length(x), ncol = length(y))

for(i in 1:length(x)) {
  for(j in 1:length(y)) {
    expected[i, j] = dmvnorm(c(x[i], y[j]), mean = c(1, 2), 
                             sigma = matrix(c(1, 0.6, 0.6, 1), 2, 2))
  }
}

par(family = font_family_global)
par(mfrow = c(1, 2))
par(mar = c(2, 1, 2, 1))
# c(0, 1, 0, 1)
persp(expected, xlab = "Evidence (x)", ylab = "Evidence (y)", 
      zlab ="", theta = 0, phi = 40, r = 3, box = T, axes = F,
      main = "")

par(mar = c(5.1, 4.1, 4.1, 2.1))
 
contour(x, y, expected, lty = 1, levels = c(0.05), main = "", 
        xlab = "Evidence (x)", ylab = "Evidence (y)", drawlabels = F,
        axes = F, lwd = lwd_global)
axis(side = 1); axis(side = 2)

points(1, 2, pch = 19)
lines(c(1, 1), c(-2, 2), lty = 2, lwd = lwd_global)
lines(c(-2, 1), c(2, 2), lty = 2, lwd = lwd_global)

@
\caption{Two projections of a two-dimensional normal distribution with the parameters $\mu = [1.0, 2.0]$, $\rho = 0.6$.} 
\label{fig:2dimnorm}
\end{figure}

\subsubsection{Calculating $d'$ in two dimensions}

For two-dimensional stimuli, two $d'$ values have to be calculated, on for each dimension. Similar to SDT, the $d'$ values are the location of the evidence distribution, but now the location has to be determined in the two-dimensional space on the $x$ and $y$ axes separately: for example, how much evidence there is for a difference in pitch AND how much is there evidence for a difference in timbre. This implies that now there are two $d'$ functions, and consequently, two of each parameter: 

\begin{align}
\label{eq:twodimdprime}
\begin{split}
d'_x &= (\frac{S_x}{\sigma_x})^{\beta_x}\\
d'_y &= (\frac{S_y}{\sigma_y})^{\beta_y}
\end{split}
\end{align}

However, since the main motivation in GRT is to model interactions between the dimensions, this would be rather incomplete. I will first discuss how response probabilities are found in GRT, and then discuss the types of interference included in the models in this thesis.

\subsubsection{Modelling responses in two dimensions}
\label{sec:modelling_responses}

Whereas in SDT the participant is usually required to make a single decision on a single axis, in GRT the participant is required to make one decision per axis; in the two-dimensional case two decisions: did the signal level change on $x$ axis and did the signal level change on $y$ axis (a similar approach has been used also by \cite{wickens1992}). 

\paragraph{Yes/No task in two dimensions}

\begin{figure}
\begin{center}
<<echo=F, fig = T, fig.height = 3.5, fig.width = 3.5>>=
y = seq(-2, 3, length.out = 50)
x = seq(-2, 3, length.out = 50)

s00 = matrix(NaN, nrow = length(x), ncol = length(y))
s11 = matrix(NaN, nrow = length(x), ncol = length(y))

for(i in 1:length(x)) {
  for(j in 1:length(y)) {
    s00[i, j] = dmvnorm(c(x[i], y[j]), mean = c(0, 0), sigma = matrix(c(1, 0.2, 0.2, 1), 2, 2))
    s11[i, j] = dmvnorm(c(x[i], y[j]), mean = c(1.00, 1.25), sigma = matrix(c(1, 0.2, 0.2, 1), 2, 2))
  }
}

par(family = font_family_global)
par(mfrow = c(1, 1))

contour(x, y, s00, xlab = "Evidence (x)", ylab = "Evidence (y)",
        main = "", levels = c(0.05), drawlabels = F, axes = F)
contour(x, y, s11, levels = c(0.05), drawlabels = F, add = T, lty = 2)

axis(side = 1); axis(side = 2)
abline(h = 0.7, lty = 2); abline(v = 0.7, lty = 2)
text(-1.6, -1.8, "(0, 0)"); text(2.6, -1.8, "(1, 0)"); text(-1.6, 2.8, "(0, 1)"); text(2.6, 2.8, "(1, 1)")

@
\end{center}
\caption{In two dimensions, the evidence distribution--when binary responses are required--is separated into four response regions.}
\label{fig:GRTsimple}
\end{figure}

Figure \ref{fig:GRTsimple} shows how the two-dimensional space is separated into response regions analogously to how it was shown from SDT Figure \ref{fig:SDT}. The distribution represented by the solid line has $d' = [0.00, 0.00]$ and the distribution represented by the dashed lines $d' = [1.00, 1.25]$. The dashed straight lines are the criteria: the vertical line is criterion for the dimension $x$ and the horizontal for dimension $y$. These criteria divide the two-dimensional space into four response regions. Most of the solid lined distribution falls into the region labelled $(0, 0)$ indicating high probability of a negative response on both dimensions. As the signal level on both dimensions is increased, as is done for the dashed line distribution, more of the distribution falls into the other regions, in this case the region $(1,1)$.  

Again, similar to unidimensional SDT, the response probabilities are found by integrating over the evidence distribution, split into response regions at the decisional boundaries as shown in Figure \ref{fig:GRTsimple}. For example the probability of observing a positive response on both dimensions is found by  integrating the two-dimensional normal distribution from the response criteria to positive infinity (region (1,1)). Whereas in the unidimensional case two psychometric functions were needed to model the two response categories, here four are needed, for each of the response possibilities:

\begin{align}
\begin{split}
\label{eq:probs}
\psi(\bm{S}; \theta)_{(\text{No}, \text{No})}  = \int_{-\infty}^{C_x} \int_{-\infty}^{C_y} N(\bm{\mu}, \bm{\Sigma})
\\
\psi(\bm{S}; \theta)_{(\text{Yes}, \text{No})}  = \int_{C_x}^{\infty} \int_{-\infty}^{C_y} N(\bm{\mu}, \bm{\Sigma})
\\
\psi(\bm{S}; \theta)_{(\text{No}, \text{Yes})}  = \int_{-\infty}^{C_x} \int_{C_y}^{\infty} N(\bm{\mu}, \bm{\Sigma})
\\
\psi(\bm{S}; \theta)_{(\text{Yes}, \text{Yes})}  = \int_{C_x}^{\infty} \int_{C_y}^{\infty} N(\bm{\mu}, \bm{\Sigma})
\end{split}
\end{align}

Here $\bm{S}$ is a vector containing the physical signal levels on the individual dimensions, $\bm{\mu}$ corresponds to the vector of $d'$-values. $\bm{\Sigma}$ is the correlation matrix, in which correlated evidence is modelled by treating the correlation coefficient as a free parameter:

$$
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
$$

Similar to SDT, the psychometric functions are most practically thought about in terms of the (bivariate) standard normal cumulative distribution function, $\phi_2(u_x, u_y, \rho) = \int_{-\infty}^{u_x} \int_{-\infty}^{u_y} N([0,0], \bm{\Sigma})$, in which $\bm{\Sigma}$ is defined as before. The upper integration limits can be found by calculating:

\begin{align*}
\begin{split}
u_x &= -c_x + d'_x \\
u_y &= -c_y + d'_y
\end{split}
\end{align*}

Then, if responses are coded with $\text{No} = -1$ and $ \text{Yes} = 1$, inputs for the $\phi_2$ function are:

\begin{align*}
\begin{split}
\psi_2(\bm{S}; \theta)_{\text{(-1, -1)}} &= \phi_2(-1 u_x, -1 u_y, \rho [-1 * -1]) \\
\psi_2(\bm{S}; \theta)_{\text{(1, -1)}}  &= \phi_2(\phantom{-}1 u_x, -1 u_y, \rho [\phantom{-}1 * -1]) \\
\psi_2(\bm{S}; \theta)_{\text{(-1, 1)}}  &= \phi_2(-1 u_x, \phantom{-}1 u_y, \rho [-1 * \phantom{-}1]) \\
\psi_2(\bm{S}; \theta)_{\text{(1, 1)}}   &= \phi_2(\phantom{-}1 u_x, \phantom{-}1 u_y, \rho [\phantom{-}1 * \phantom{-}1]) 
\end{split}
\end{align*}

This process can be defined generally with the equation:

\begin{equation}
\psi_2(\bm{S}; \theta)_{\bm{R}} = \phi_2([-c_x + d'_x]r_x, [-c_y + d'_y] r_y, \rho [r_x r_y])
\label{eq:generalPfun}
\end{equation}

in which the $\bm{R} = [r_x r_y]$. All of the Stan \citep{stan2019} programs in the appendices assume that responses are coded in this way.

\paragraph{2I-4AFC}

In the SDT section I discussed the 2I-2AFC task. Here I expand it to the 2I-4AFC task. There are still two temporal intervals, as was shown in the schematic in Figure \ref{fig:YesNoAfc}, but now the participant has four response alternatives, since either signal could be in either interval: for example that pitch changed in the first interval and timbre in the second. The same dimension never changes in both intervals, that is, if there's a pitch change in the first interval, there never is a pitch change in the second interval.

Similar to Figure \ref{fig:2AFC}, Figure \ref{fig:2I4AFC} shows the distributions of evidence from the two intervals. Note that each of the signals could be in either of the intervals, so there are four possible arrangements for the signals, but similar to the case in 2I-2AFC it is not important in which intervals the signals are concretely, what is important is the difference between the signal and noise distributions. 

It is assumed in Figure \ref{fig:2I4AFC} that $\kappa_{x}^{\mu}$ parameter is non-zero, which means that signal on $y$-dimension is able to increase the $d'$-value on the $x$-dimension--the interaction parameters, which $\kappa_{mu x}$ is one of, will be defined more comprehensively in the next section. This can be seen from the fact that in the left panel the mean of the noise distribution on the $x$-axis is slightly greater than zero. If such interaction was absent, the mean of the noise distribution would be zero on both dimensions, there would be on average no evidence for change in either dimension. The interaction shrinks the difference between the signal and noise distributions on that dimension.

As before, the participant is thought to base their decision on a decision variable that represents the difference of evidence in the intervals. As a consequence the $d'$ values on both dimensions have to be divided by $\sqrt{2}$ (cf. section on 2I-2AFC). The distribution of the decisional variable can be seen in the right panel of Figure \ref{fig:2I4AFC}. The thick black lines mark boundaries between \textit{hits} and \textit{false alarms}. The participant, in this particular scenario, is very likely to score a \textit{hit} on the $y$-axis, but on $x$-axis they are almost as likely to pick the wrong interval as the correct; the most likely response pairs being (FA, Hit) and (Hit, Hit).

\begin{figure}
\centering
<<fig=T,echo=F,fig.height=4>>=

par(family = font_family_global)
par(mfrow = c(1, 2))

# Left panel

plot(NULL, xlim = c(-3, 5), ylim = c(-3, 5), axes = F, xlab = "Evidence (x)", ylab = "Evidence (y)",
     main = "Evidence contained in the intervals")
axis(side = 1); axis(side = 2)
abline(h = seq(-3, 5, 1), lty = 3, col = rgb(0, 0, 0, 0.5))
abline(v = seq(-3, 5, 1), lty = 3, col = rgb(0, 0, 0, 0.5))
drawBivarDistr(c(0.75, 0), 1, F)
drawBivarDistr(c(1, 2), 1, T)

text(0.75, -2.2, "Noise interval")
text(1.00, 4.2, "Signal interval")

# Right panel

plot(NULL, xlim = c(-3, 5), ylim = c(-3, 5), axes = F, xlab = "x", ylab = "y",
     main = "Decisional variable")
axis(side = 1, at = c(-3, -1, 2, 4), labels = c("", "FA", "Hit", ""))
axis(side = 2, at = c(-3, -1, 2, 4), labels = c("", "FA", "Hit", ""))
abline(h = seq(-3, 5, 1), lty = 3, col = rgb(0, 0, 0, 0.5))
abline(v = seq(-3, 5, 1), lty = 3, col = rgb(0, 0, 0, 0.5))

drawBivarDistr(c(0.25, 2), sqrt(2), F)
abline(h = 0, lwd = 2)
abline(v = 0, lwd = 2)

@
\caption{In the 21-4AFC task, the participant is assumed to base their responses on the difference in the evidence from signal and noise distributions (left panel). Variance of the distribution of differences is th sum of the variances of the individual distributions (right panel).}
\label{fig:2I4AFC}
\end{figure}

As in the Yes/No task, responses can be coded with -1 and 1. Here, \textit{false alarms} are coded with -1 and \textit{hits} with 1.  The general expression for the psychometric function is then

\begin{equation}
\psi_2(\bm{S}; \theta)_{\bm{R}} = \phi_2(\frac{d'_x}{\sqrt{2}}r_x, \frac{d'_y}{\sqrt{2}} r_y, \rho [r_x r_y])
\label{eq:generalPfun2}
\end{equation}

\paragraph{Comparison of the Yes/No and 2I-4AFC tasks}

The main difference between the Yes/No and 2I-4AFC tasks is what the participant's decision is directly related to. In the Yes/No task the decision is about the \textit{appearance} of the stimulus. As was already discussed, this means that the performance of the observer cannot be decoupled from their decisional criterion. This adds one more parameter to be estimated from the data. 

In the 2I-4AFC task the decision about the \textit{interval}. This means that it is not necessary for the participant to adopt a criterion relating the strength of evidence to a response. Also, since the participants are choosing an interval, which is by its nature more neutral decision, it is usually thought that this kind of task is less susceptible to bias \citep[Chapter 6]{kingdomprins2010}.

Since on each trial two stimuli have to be presented in the 2I-4AFC task, it is somewhat slower to administer. However, if the observer knows, in the Yes/No task, that some non-zero signal is present on each trial, nothing prevents them from answering \textit{yes} on each trial--they will always be correct, and seemingly able to detect the faintest of signals. To counter this, some amount of \textit{catch} trials are usually included. These are trials during which the signal level is zero. During these trials no information about the sensory processing--the parameters $\sigma$ and $\beta$--is accumulated. So even though individual trials are faster to complete, there has to be more of them. 

The issue of catch trials is exacerbated in the multidimensional case. This is because in the one-dimensional the issue is simple solved by including some noise stimuli as catch trials in the experimental run, resulting in two kinds of stimuli being presented: noise and signal stimuli. In the two-dimensional case there has to be three kinds of catch trials: noise only stimulus on either of the dimensions or both. Otherwise if the participant became aware of e.g. that the adaptive procedure only rarely selects a stimulus in which there's a noise stimulus on one dimension--i.e. [0,S] or [S,0] --, they might become biased towards choosing the responses [No, No] and [Yes,Yes]. The linear decision bounds used in this work are not able to model this kind of response bias; an issue which will be discussed later. 

Then, on theoretical grounds, the only difference to be expected is the omission of the criterion parameters from the 2I-4AFC model--reduction in sensitivity is taken into account by incorporating the term $\sqrt{2}$. 

\subsubsection{Interactions between the dimensions}

As already discussed GRT aims to model \textit{interactions} between the dimensions. When the one dimension has an effect on the other, this is called \textit{interference}. There are three kinds of interactions considered in this thesis. Two of these interactions are defined as \textit{couplings} between the psychometric functions, and the third interaction is the correlation of the evidence distribution, but since including correlation coefficient was already conclusively discussed in the preceding section, I will only discuss the couplings here.

Two kinds of couplings are modelled. These are demonstrated graphically  in Figure \ref{fig:grt_couplings}. Panels in the left column demonstrate coupling between means while panel on the right side demonstrate coupling between standard deviations. Top row shows effects of the interactions on the evidence distributions while the bottom row shows the effect on the psychometric function on one dimension.\footnote{mathematically the union of the psychometric functions $\psi{\bm{S}}_{(\text{Yes}, \text{Yes})}$ and $\psi{\bm{S}}_{(\text{Yes}, \text{No})}$.} In all panels dashed lines show the situation in the absence of interaction while the solid lines are drawn in the presence of interaction. 

\begin{figure}
\begin{center}
<<echo=F, fig=T, fig.height = 6.0>>=
library(mvtnorm)
y = seq(-2, 4, length.out = 50)
x = seq(-2, 4, length.out = 50)
expected = matrix(NaN, nrow = length(x), ncol = length(y))
meanshift = matrix(NaN, nrow = length(x), ncol = length(y))
varshift = matrix(NaN, nrow = length(x), ncol = length(y))

for(i in 1:length(x)) {
  for(j in 1:length(y)) {
    expected[i, j] = dmvnorm(c(x[i], y[j]), mean = c(1, 0), sigma = matrix(c(1, 0, 0, 1), 2, 2))
    meanshift[i, j] = dmvnorm(c(x[i], y[j]), mean = c(1, 1), sigma = matrix(c(1, 0, 0, 1), 2, 2))
    varshift[i, j] = dmvnorm(c(x[i], y[j]), mean = c(1, 0), sigma = matrix(c(4, 0, 0, 1), 2, 2))

  }
}

par(family = font_family_global)
par(mfrow = c(2, 2))

contour(x, y, expected, lty = 2, levels = c(0.05), main = expression(kappa[mu]),
        xlab = "Evidence (x)", ylab = "Evidence (y)", drawlabels = F, axes = F, lwd = lwd_global)
axis(side = 1); axis(side = 2)
contour(x, y, meanshift, add =T, levels = c(0.05), drawlabels = F, lwd = lwd_global)
points(1, 0, pch = 4)
points(1, 1, pch = 3)

contour(x, y, expected, lty = 2, levels = c(0.05), main = expression(kappa[sigma]),
        xlab = "Evidence (x)", ylab = "Evidence (y)", drawlabels = F, axes = F, lwd = lwd_global)
axis(side = 1); axis(side = 2)
contour(x, y, varshift, add =T, levels = c(0.05), drawlabels = F, lwd = lwd_global)
points(1, 0, pch = 4)
points(1, 0, pch = 3)


##

curve(pnorm(-1.2 + x/2), 0, 10, lty = 2, axes = F, 
      xlab = "Signal strength", ylab = "P(Yes)")
axis(side = 1); axis(side = 2)
curve(pnorm(-1.2 + x/2 + 0.5), 0, 10, add = T)

curve(pnorm(-1.2 + x/2), 0, 10, lty = 2, axes = F,
      xlab ="Signal strength", ylab = "P(Yes)")
axis(side = 1); axis(side = 2)
curve(pnorm(-1.2/5 + x/5), 0, 10, add = T)

@
\end{center}
\caption{The two kinds of couplings in the model. The contours in the top row depict bivariate normal distributions. Dashed lines indicate the shape and location of the distribution in the absence of interaction; the solid lines depict them in the case that interaction is present. Curves in the lower row are psychometric functions for a single dimension.}
\label{fig:grt_couplings}
\end{figure}

\paragraph{Coupled means, $\kappa_{\mu}$}

Coupling between the means of the distributions is modelled by letting the other dimension influence the $d'$ values: 

\begin{align}
\label{eq:kappa_mu}
\begin{split}
d'_x &= (\frac{S_x}{\sigma_x})^{\beta_x} + \kappa_x^{\mu} S_y \\
d'_y &= (\frac{S_y}{\sigma_y})^{\beta_y} + \kappa_y^{\mu} S_x
\end{split}
\end{align}

As can be seen from the lower left panel of Figure \ref{fig:grt_couplings} this kind of interactions shifts the psychometric on the affected dimension. Here, the presence of interference has increased the $d'$ values and consequently probabilities for positive responses all along the range of the function.

\paragraph{Coupled standard deviations, $\kappa_{\sigma}$}

Another possibility is that the standard deviations are coupled. This kind of situation can arise if for example the other dimension is length: shorter signals contain less information, thus making frequency judgments noisier (see \citet[p.274]{houtsma1995}, also worse accuracy with shorter signal times has been reported by e.g. \cite{townsend1988} implying more noise).

Similar to $\kappa_{\mu}$, terms are included for letting the other dimension influence the $\sigma$ parameter. However, since the $\sigma$ parameters are constrained to positive real numbers (standard deviation can't be negative), this interaction is defined on the logarithmic scale (for a similar approach in probit modelling, which is closely related to SDT as will be discussed shortly, see \citet{freeman2018}):

\begin{align}
\label{eq:varshift}
\begin{split}
\sigma_x &= exp(log\sigma_x + \kappa_x^{\sigma} S_y) \\
\sigma_y &= exp(log\sigma_y + \kappa_y^{\sigma} S_x)
\end{split}
\end{align}

When the model parameters relating to response boundaries or criteria--as is the case in here for the \textit{Yes/No} model--they also have to be divided by the standard deviation. This is intuitively clear from the observation that as the spread of the evidence distribution becomes larger, the parts of the evidence distribution falling into the response regions become increasingly equal. In other words response probabilities get closer to 0.25, and a larger denominator for the term $c$ will achieve just this:

\begin{equation}
\psi_2(\bm{S}; \theta)_{\bm{R}} = \phi_2([-\frac{c_x}{\sigma_x} + d'_x]r_x, [-\frac{c_y}{\sigma_y} + d'_y] r_y, \rho [r_x r_y])
\label{eq:generalPfun_varshift}
\end{equation}  

This means that the definition of criterion is a bit different from that of when no coupling between standard deviations is present. In the model with constant variance, the criterion corresponds to the Z-score of the false alarm probability (see \citet[Chapter 6]{kingdomprins2010}): $\text{criterion} = \phi^{-1}(P(FA))$, in which $\phi^{-1}$ is the standard normal inverse cumulative distribution function; in the model with coupled standard deviations the whole first term corresponds to the Z-score of false alarm probability: $\frac{\text{criterion}}{\sigma} = \phi^{-1}(P(FA))$. This means that in this model the criterion parameter is more accurately the geometric location of the decisional criterion in the decisional space, independent of the variance of the evidence distributions. However, since it is easier to interpret false alarm probabilities, prior in the models is placed on the term $\frac{\text{criterion}}{\sigma}$--what this means in practice will be discussed in the section on Bayesian statistics

From the lower right panel of Figure \ref{fig:grt_couplings} one can see that the effect is to draw the response probabilities towards 0.5 on a single dimension. Note that this is in agreement with what was said earlier about response probabilities being drawn towards 0.25: since the psychometric function in the figure is shown only for one dimension, it represents the probability of a positive response irregardless of the other dimension. 

\paragraph{Model with both kinds of couplings}

The most realistic case is one in which both kinds of couplings are included in the model. This is achieved easily by using Equation \ref{eq:kappa_mu} and calculating the $\sigma$ terms as in Equation \ref{eq:varshift}.

\paragraph{Note on terminology} 

The couplings, $\kappa_{\mu}$ and $\kappa_{\sigma}$, are sometimes called \textit{between stimuli/distributions interference} \citep{silbert2009} and the situation in which they are absent can be referred to as \textit{sensory independence} \citep{ashby2015}. Correlation can be called \textit{within stimulus/distribution interference} \citep{silbert2009} and the absence of correlation as \textit{perceptual independence} \citep{ashby2015}. In addition, $\kappa_{\mu}$ can be called \textit{mean-shift integrality} and $\kappa_{\sigma}$ \textit{variance-shift integrality} \citep{ashby1994}.

\subsection{Lapsing behaviour}
\label{sec:lapses_general}

It is possible that during a psychophysical task, people sometimes \textit{lapse}. That is, for some reason the response they give doesn't reflect the cognitive process of interest. This might be due to e.g. lapse in attention, a coding error in the program, or a slip of a finger. In the classic GRT models, lapsing behaviour has not been taken into account, even though in the unidimensional case lapses have been shown to be able to exert considerable bias on the parameter estimates of the psychometric function \citep{wichmannhill2001}. 

Estimating lapsing rate from data can be problematic (\cite{wichmannhill2001, treutwein1999}), and this problem can be worse for adaptive procedures \citep{prins2012}. Since the values of the psychometric function that are closer to zero are affected both the lapses and the decision criterion (in the Yes/No task), most of the information gained about lapsing behaviour comes from lapses happening at high intensities. What makes matters worse is that the lapsing response doesn't necessarily differ from what the participant would've answered otherwise, so the distinction between lapses and genuine response is ambiguous (for further discussion on these problems, see \cite{prins2012}). For these reasons I fixed the values of $\lambda$ and $\gamma$ during the adaptive estimation phase. However, during the analysis of the psychophysical data I estimated the parameter $\lambda$ from the data, a decision, which will be discussed in more detail in the analysis section.

I model lapsing behaviour by using a (hierarchical) mixture model (see \citet{zeigenfuse2010}), which will be discussed in Section \ref{sec:hierarchical_models} \textit{\nameref{sec:hierarchical_models}}.
