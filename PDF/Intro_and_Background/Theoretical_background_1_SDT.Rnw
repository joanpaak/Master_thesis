%!Rnw root = ../Joni_Paakko_-_Thesis.Rnw

\section{Theories of Detection}
\label{sec:GRT}

The central theoretical concept in this work is General Recognition Theory (GRT), which is essentially a generalization of Signal Detection Theory (SDT) to multiple dimensions, with a focus on modelling interactions between them. Due to this hierarchy, I will first introduce the reader to SDT, before discussing GRT.

Both SDT and GRT are discussed in the context of a \textit{discrimination task}. In such a task a single stimulus consists of two components: the \textit{reference tone} and the \textit{test tone}, as shown schematically  in Figure \ref{fig:discrtask}. If for example the dimension of interest is pitch, the reference tone would always be the same, e.g. 150 Hz, and the test tone would either be the same or a higher pitch. The participant would then have to determine if there was a difference between the test and reference tones.

\begin{figure}[!htb]
\centering
\includegraphics[scale = 0.5]{DiscriminationTask}
\caption{Schematic illustration of a stimulus in a discrimination task.}
\label{fig:discrtask}
\end{figure}

A stimulus in which the difference between reference and test tones is zero is called a \textit{noise} stimulus while one in which the difference is greater than zero is called a \textit{signal} stimulus; alternatively these can be called just noise and signal. 

\subsection{Signal Detection Theory}

Continuing the example of a  pitch discrimination task, during such experiment it is not rare to observe the participant sometimes give a negative and sometimes a positive response to the same stimulus on different presentations. The view that SDT takes is that the responses are not based directly on the physical signals, but rather some latent (unobserved) internal quantity, sometimes referred to as \textit{evidence} \citep{wickens2002} or \textit{judgments} \cite{stigler2003}, I will be using the term \textit{evidence} from here on. It is thought that the latent amount of evidence for e.g. the test tone being higher than the reference tone is subject to random variation, which in turn leads to variation in responses (\citet[p. 154]{kingdomprins2010}, \citet[p. 11]{wickens2002}). These random perturbations to the amount of evidence are commonly thought to arise primarily from sensory sources, but I will discuss this issue later in greater detail.

Some assumptions about the nature of the perturbations are usually made, for example that small perturbations are more common than large ones, and that the average amount of error is zero. A popular choice for modelling the distribution of the perturbations is the normal distribution with mean 0 and unknown variance. This is also the assumption that I will be making in this thesis. By making this assumption about the distribution of the perturbations, one can calculate the predicted response probabilities for different stimuli, and consequently make statistical inferences about the sensory processes of interest. Rest of the discussion will deal with the minutiae of these calculations. 

\subsubsection{Relationship between stimulus and $d'$}

Since the amount of evidence is assumed to be random, it can be represented as probability distributions as in Figure \ref{fig:SDT}. In the figure $S$ stands for physical signal level, for example the difference in frequencies between the reference and test tones. As was discussed in the previous section, normal distribution is used to model the distribution of evidence. 

If the subject is presented with a noise stimulus (one in which $S = 0$), the amount of evidence on any single trial is assumed to be a random sample from the zero-centred distribution in Figure \ref{fig:SDT}; in the context of the pitch discrimination task this would correspond with stimulus in which the test and reference pitches are identical. As $S$ is increased, the distribution from which the evidence is assumed to be sampled from shifts rightward, farther from zero, as is shown for $S = 2$ and $S = 4$.

\begin{figure}[!htb]
\begin{center}
<<echo=F, fig=T, fig.height=4>>=

par(family = font_family_global); par(mfrow = c(1, 1))

sigma = 1.5

curve(dnorm(x, 0, sigma), -4, 12, axes = F, xlab = "Evidence", ylab = "", 
      ylim = c(0, 1), lwd = lwd_global, main = "")

x_lines = seq(-2, 10, 2)

for(i in 1:length(x_lines)){
  lines(c(x_lines[i], x_lines[i] + 5), c(0, 0.5), lty = 2, lwd = lwd_global,
        col = rgb(0, 0, 0, 0.5))  
}

axis(side = 1, lwd = lwd_global, line = -0.45)

curve(dnorm(x, 4, sigma) + 0.2, -1, 12, add = T, lwd = lwd_global)
curve(dnorm(x, 8, sigma) + 0.4, 1, 12, add = T, lwd = lwd_global)

x_1 = seq(-4, 12, 0.01)
x_2 = seq(-1, 12, 0.01)
x_3 = seq(1, 12, 0.01)

y_1 = dnorm(x_1, 0, sigma)
y_2 = dnorm(x_2, 4, sigma)
y_3 = dnorm(x_3, 8, sigma)


polygon(c(x_3, rev(x_3)), c(y_3, rep(0, length(x_3)))  + 0.4, 
        col = highlight_col, border = NA)

polygon(c(x_2, rev(x_2)), c(y_2, rep(0, length(x_2)))  + 0.2, 
        col = highlight_col, border = NA)

polygon(c(x_1, rev(x_1)), c(y_1, rep(0, length(x_1)))  + 0.0, 
        col = highlight_col, border = NA)

text(0, 0.4, "S = 0", cex = 1.2)
text(4, 0.6, "S = 2", cex = 1.2)
text(8, 0.8, "S = 4", cex = 1.2)

@
\end{center}
\caption{Evidence, according to SDT, is a random  variable. It can be assumed to be normally distributed, with monotonically increasing $\mu$ as a function of signal strength, $S$, and constant variance, $\sigma$.}
\label{fig:SDT}
\end{figure}

Signal level divided by the standard deviation of evidence is called $d'$, usually interpreted as \textit{discriminability} \citep[Chapter 6]{kingdomprins2010} or \textit{signal-to-noise ratio} \citep{kontsevichtyler1999}. Standardized in this way, the standard deviation of evidence is always one, and the mean of the evidence distribution is $d'$.

Relationship between $d'$ and $S$ is not necessarily linear. Theoretically this kind of non-linearity could  arise e.g. from changes in the variance of the evidence distribution. However, the non-linearity is often interpreted as being the result of non-linearity in signal transduction, i.e. the change of physical signal into internal.\footnote{Familiar example of non-linear transduction is how pitch is experienced in large scale: in order to move one octave up on the psychological scale of pitch, one has to move increasingly fast on the physical frequency, see \citet[Chapter 5]{zwickerfastl}.}

The functional relationship between physical signal strength and $d'$ has been quantified in many different ways in the psychophysical literature (for a selection, see \citet[Appendix A]{lesmes2015}). The functional form used here is based on the widely used \textit{power law} model of signal transduction (see e.g. \citet{kontsevichtyler1999, dai2011, lesmes2015}):

\begin{equation}
d' = (\frac{S}{\sigma})^\beta
\label{eq:dprimefunc}
\end{equation}

Here $S$ is again the physical signal strength, $\sigma$ corresponds to the standard deviation of evidence and $\beta$ models non-linearity between signal level and signal-to-noise ratio. Note that often the letter $\alpha$ is used for the standard deviation of evidence (e.g. in \cite{dai2011, kontsevichtyler1999, kingdomprins2010}), but since this parameter is directly related to the standard deviation of the evidence distribution, I feel $\sigma$ is more appropriate. 

The effects of changing these parameters are shown in Figure \ref{fig:dprimefunc}. The smaller $\sigma$ is, the faster $d'$ increases. $\beta$ parameter affects the linearity of the function: when $\beta < 1$ the function increases faster than the linear case before the point at which $d' = 1$ and slower after that; the opposite happens when $\beta > 1$. 

\begin{figure}[!htb]
\begin{center}
<<fig=TRUE, echo = FALSE, fig.height=4>>=

par(mfrow = c(1, 2))
par(family = font_family_global)

# Right panel:

curve((x/2), 0, 7, ylab = "d'", xlab = "Signal strength", axes = F)
axis(side = 1); axis(side = 2)
curve((x/4), 0, 7, add = T, lty = 2)
text(5, 3, expression(paste(sigma, " = 2")))
text(5, 1.6, expression(paste(sigma, " = 4")))

# Left panel:

curve((x / 2) ^ 1, 0, 5, xlab = "Signal strength", ylab ="d'", axes = F)
axis(side = 1); axis(side = 2)
curve((x / 2) ^ 0.4, 0, 5, add = T, lty = 2)
curve((x / 2) ^ 1.6, 0, 5, add = T, lty = 4)

text(3.5, 1.0, expression(paste(beta, " = 0.4")))
text(4.2, 1.8, expression(paste(beta, " = 1")))
text(2.5, 2.0, expression(paste(beta, " = 1.6")))
@
\end{center}
\caption{ The effect of $\sigma$ and $\beta$ parameters on the $d'$ function. Left panel shows the effect of changing $\sigma$ parameter while keeping $\beta$ constant ($\beta = 1$) and the right panel shows the effect of changing $\beta$ parameter while keeping $\sigma$ constant ($\sigma = 2$).}
\label{fig:dprimefunc}
\end{figure}

\subsubsection{Modelling responses: relationship between $d'$ and response}

The preceding discussion about relating $S$-values to $d'$-values is just one half of SDT, there has to be a way of relating the $d'$-values to responses, $R$. Usually categorical decisions are required from the participant, and the specific response categories depend on what kind of task they are performing. I will be considering a Yes/No and a 2-interval 2-alternative forced choice (2I-2AFC) tasks. These are shown schematically in Figure \ref{fig:YesNoAfc}.

\begin{figure}[!htb]
\centering
\includegraphics[scale = 0.5]{YesNoAfc}
\caption{Schematic representations of the Yes/No (panel a) and 2-interval forced choice (panel b) tasks. \textit{S = } signal, \textit{R =} response.}
\label{fig:YesNoAfc}
\end{figure}

In the Yes/No task (panel \textit{a)} of Figure \ref{fig:YesNoAfc}) the participant's task is to indicate if they detected the signal. The name of the task is quite literal as the participants provide \textit{Yes} and \textit{No} answers. As shown in the figure, the participant is first presented with the stimulus after which they are expected to give their answer.

In contrast to this, in the 2I-2AFC task the participant is presented with two \textit{observation intervals}, interval referring here to a temporal interval as can be seen from panel \textit{b)} of Figure \ref{fig:YesNoAfc}. During each interval the participant is presented with a stimulus, one containing only noise and the other containing the signal. The participant's task is then to indicate which interval they thought contained the signal. Again, the name is quite literal: the participant is presented with two \textit{intervals} and they have to choose their response among two \textit{alternatives}.

\paragraph{Responses in the Yes/No task}

Decisional processing in the Yes/No task is modelled by assuming that the participant has an internal criterion for the evidence. When evidence is below this criterion the participant will respond negatively and when evidence exceeds the criterion they will respond  positively.

When the participant is presented with a signal stimulus, positive responses are called \textit{hits}; when they  are presented with a noise stimulus, positive responses are called \textit{false alarms}. \citep{wickens2002, kingdomprins2010}. 

Criterion is represented by the red dashed vertical lines in Figure \ref{fig:yesno}. As signal level increases, and as a consequence $d'$, the evidence distribution shifts to the right. Since the criterion is fixed, larger portion of the evidence distribution falls to the right of the criterion, indicating higher probability of a positive response.

\begin{figure}[!htb]
\centering
<<echo=F, fig=T,fig.height=3.5>>=
par(family = font_family_global)
par(mfrow = c(1, 2))
crit = 1.2

# Left panel

curve(dnorm(x, 0, 1), -3, 9, xlab = "Evidence", 
      ylab = "Density", ylim = c(0, 0.4), 
      main = "d' = 0",
      lwd = lwd_global, axes = F)
abline(v = crit, lty = 2, col  = "black", lwd = lwd_global)
axis(side = 1); axis(side = 2)

x = seq(crit, 9, length.out = 100)
y = dnorm(x, 0, 1)
polygon(x = c(x, rev(x)), y =  c(rep(0, 100), rev(y)),
        col = highlight_col, border = rgb(0, 0, 0, 0))

# Right panel

curve(dnorm(x, 2, 1), -3, 9, xlab = "Evidence", 
      ylab = "Density", ylim = c(0, 0.4), 
      main = "d' = 2",
      lwd = lwd_global, axes = F)
abline(v = crit, lty = 2, col  = "black", lwd = lwd_global)
axis(side = 1); axis(side = 2)

x = seq(crit, 9, length.out = 100)
y = dnorm(x, 2, 1)
polygon(x = c(x, rev(x)), y =  c(rep(0, 100), rev(y)),
        col = highlight_col, border = rgb(0, 0, 0, 0))

@
\caption{Binary decisions in the YesNo-paradigm. The distribution of evidence is divided into response regions.}
\label{fig:yesno}
\end{figure}

The probability of evidence exceeding the decisional criterion on any trial can be found by finding the area (shaded part in Figure \ref{fig:yesno}) under the normal distribution upwards from the criterion (see \citet{wickens2002, kingdomprins2010}):

\begin{equation}
\label{eq:SDTintegral}
P(R = 1) = \int_{c}^{\infty} N(d', 1)
\end{equation}

where $N$ is the normal distribution function with parameters $\mu$ (mean) and $\sigma$ (standard deviation), $d'$ is the signal-to-noise ratio as calculated in Equation \ref{eq:dprimefunc} and $c$, the lower bound of the integral, is the decisional criterion. 

The integral in this equation is usually written in terms of the cumulative standard normal distribution function, $\phi$. The resulting function, denoted with the Greek letter $\psi$, is called the \textit{psychometric function}. \cite[Chapter 4]{kingdomprins2010}

Usually only the positive response is considered, since the probability of a negative response is simply the complement of it, but it is more principled to show the probabilities for both the positive and negative responses:

\begin{align*}
\begin{split}
\psi(S; \Theta)_{\text{Yes}} &= \phi(-c + d') \\
\psi(S; \Theta)_{\text{No}} &=  1 - \phi(-c + d')
\end{split}
\end{align*}

Here $\Theta$ is a vector containing the parameters of the psychometric function ($\sigma$, $c$ and $\beta$), and $S$ is the physical signal level. Given vectors of observed signals and responses ($S$ and $R$), one can find the best fitting values for the parameters, which then would taken as the Signal Detection Theoretical interpretation of the observer's performance. In this interpretation the performance is divided into sensory (parameters $\sigma$ and $\beta$) and decisional (parameter $c$) factors.

Since sensory and decisional processes are separate in the model, keeping the parameters that model sensory processing constant while varying the criterion will result in different predicted proportions of \textit{Yes} and \textit{No} responses and as a consequence hits and false alarms. That is to say that the perceptual processing capabilities can be exactly the same for two participants, but the observed amounts of hits and false alarms can be different, if one participant has a more lax criterion. This is shown in the left panel of Figure \ref{fig:critchange}. Here, the parameters relating to sensory factors are kept constant while the criterion is varied.

It is also important to notice that hits and false alarms are coupled. If the participant wants to avoid false alarms, they will have to adopt a stricter criterion for the evidence, and  as a consequence they  will inevitably also make less hits. This is because the aim of SDT is to model situation in which there is a non-zero probability of confusing signal with noise. 

The coupling between hits and false alarms is demonstrated in the form the a \textit{receiver operating characteristic} (ROC) curve \cite[161]{kingdomprins2010} in the right panel of Figure \ref{fig:critchange}. When the participant adopts stricter criterion, they move left on the ROC curve. When the signal is stronger ($d'$-value is higher), the participant is able to score more hits. 

Note that if the participant aims to avoid false alarms altogether, they will have to adopt a criterion that, theoretically, would also result in zero hits; the criterion has to be relaxed a bit in order for the participant to perform in the task in any meaningful way, resulting in some false alarms and hits, as just discussed. Generally, the participant is free to "set their own criterion", however it is possible to calculate the optimal value of the decisional criterion. 

\begin{figure}[!htb]
\begin{center}
<<fig=TRUE, echo = FALSE, fig.height=3.5>>=

par(mfrow = c(1, 2))
par(family = font_family_global)
# Left panel

crit = c(0.5, 1.5)
alpha = 2
beta = 1

curve(pnorm(-crit[1] + x / alpha, 0, 1), 0, 7, 
      ylab = "P(Yes)", xlab = "Signal strength", ylim = c(0, 1), axes = F)
axis(side = 1); axis(side = 2)
curve(pnorm(-crit[2] + x/alpha, 0, 1), 0, 7, add = T, lty = 2)
legend("bottomright", legend = 
         c(paste("c = ", crit[1]), paste("c = ", crit[2])), lty = c(1, 2), bty = "n")


# Right panel

crit = seq(4, -3, -0.01)
dPrime = c(0.5, 1.5)
pHit_1 = pnorm(-crit + dPrime[1])
pHit_2 = pnorm(-crit + dPrime[2])
pFa = pnorm(-crit)

plot(pFa, pHit_1, type = "l", ylab = "P(Hit)", xlab = "P(FA)", axes = F)
axis(side = 1); axis(side = 2)
points(pFa, pHit_2, type = "l", ylab = "P(Hit)", xlab = "P(FA)", lty = 2)

legend("bottomright", legend = 
         c(paste("d' = ", dPrime[1]), 
           paste("d' = ", dPrime[2])), lty = c(1, 2), bty = "n")

@
\end{center}
\caption{ Decisional processing in Signal Detection Theory. Left panel: The effect of differing criteria on the predicted amounts of false alarms and hits while the parameters describing the sensory processing are kept constant. \textit{Higher} criterion value implies stricter criterion--that the participant is less willing to respond \textit{yes}--while a \textit{lower} value implies more lax criterion. Right panel: Receiver operating characteristic (ROC) curve for two fixed $d'$-values, showing that the proportions of hits and false alarms are coupled.}
\label{fig:critchange}
\end{figure}

\paragraph{Responses in the 2I-2AFC task}

Since the participant is presented with two stimuli in the 2I-2AFC task, theoretically the participant is comparing two random numbers, one representing strength  of evidence in the first interval and the other representing strength of evidence in the second interval. From which ever interval the larger of these two was from gets picked as the \textit{signal} interval by the participant.

As a consequence the signal-to-noise ratio is decreased compared to the Yes/No task. Intuitively this can be understood by noticing that if one compares two things, both of which contain uncertainty, the individual uncertainties add up. Mathematically speaking, the participant can be thought of basing their decision on decision variable (here $dv$) that is the difference of the two random variables:

\begin{equation}
dv = [N(d', 1) - N(0, 1)] I
\end{equation}

Here $I$ is an indicator variable which  is $-1$ when the signal is in the first interval and $1$ when the signal is in the second interval. Consequently the decision rule is to respond \textit{Interval 1} when the decision variable $dv$ is negative and \textit{Interval 2} when it's positive. 

It is usually more practical, however, to think about the responses in terms of hits and false alarms, similar to the Yes/No task. This makes it possible to drop the the indicator variable from the equation, since we are only interested in the difference between the signal and noise distributions. This basic structure of the 2AFC model is demonstrated in Figure \ref{fig:2AFC}. 

\begin{figure}[!htb]
\centering
<<fig = T, echo = F>>=
par(family = font_family_global)
par(mfrow = c(2,1))

# Upper panel

curve(dnorm(x, 0, 1), -3, 8, main = "Distributions of evidence in the two intervals", 
      ylab = "Density", xlab = "Evidence", lwd = lwd_global, ylim = c(0, 0.6), axes = F)
curve(dnorm(x, 1.5, 1), -3, 8, add = T, lwd = lwd_global)

x = seq(-3, 8, 0.01)
y = dnorm(x, 1.5, 1)

polygon(c(x, rep(x)), c(y, rep(0, length(x))), col = highlight_col,
        border = NA)

lines(c(0, 0), c(dnorm(0), 0.55), lwd = lwd_global, lty = 2)
lines(c(1.5, 1.5), c(dnorm(0), 0.55), lwd = lwd_global, lty = 2)
lines(c(0, 1.5), c(0.55, 0.55), lwd = lwd_global, lty = 2)
text(0.75, 0.60, "d'")

text(3.90, 0.3, "Signal interval")
text(-2.2, 0.3, "Noise interval")

axis(side = 1); axis(side = 2)

# Lower panel

curve(dnorm(x, 1.5, sqrt(2)), -3, 8, main = "Distribution of the decision variable",
      ylab = "Density", xlab = "Sum of evidence", lwd = lwd_global, axes = F)

lines(c(0, 0),  c(0, dnorm(0, 1.5, sqrt(2))), lwd = lwd_global, lty = 2)
axis(side = 1); axis(side = 2)

x = seq(0, 9, length.out = 100)
y = dnorm(x, 1.5, sqrt(2))
polygon(x = c(x, rev(x)), y =  c(rep(0, 100), rev(y)),
        col = highlight_col, border = rgb(0, 0, 0, 0))
text(1.4, 0.1, "P(Hit)")
text(-2, 0.1, "P(False alarm)")

@
\caption{How responses are generated in the forced choice task. In the upper panel, the shaded distribution shows distribution of evidence from the signal interval while the non-shaded distribution shows evidence from the noise interval. The lower panel shows the distribution of the decision variable. }
\label{fig:2AFC}
\end{figure}

In the upper panel the zero-centred distribution represents evidence from the noise-only interval while the distribution centred on 1.5 represents evidence from the signal interval. Irregardless of whether the signal is in the first or second interval, the signal interval will be shifted $d'$ amount. Since the probability of making the correct decision is the proportion of the decision variable distribution that exceeds 0, shifting the signal distribution to the right will increase this probability--as one would expect.

What is important to note is that the decision variable is the difference of two normally distributed random variables, both with standard  deviation of one, since they represent the standardized quantity $d'$. It follows from the additivity of variances, and from the fact that standard deviation is the square root of variance, that the standard deviation of the decision variable is $\sqrt{2}$--remember, that the $d'$ values are standardized  so that the standard deviations of the individual evidence distributions are always one, so the total standard deviation here is $\sqrt{1^2 + 1^2} = \sqrt{2}$. It follows then that the $d'$ values in the psychometric function have to be divided by this constant:

\begin{align*}
\begin{split}
\psi(S; \Theta)_{\text{Hit}} &= \phi(\frac{d'}{\sqrt{2}}) \\
\psi(S; \Theta)_{\text{False alarm}} &=  1 - \phi(\frac{d'}{\sqrt{2}})
\end{split}
\end{align*}

The term $c$ is dropped since it is assumed that, on average, the observer isn't biased towards either of the intervals. Some bias is probably not detrimental: any such bias would presumably be most noticeable when the signal level is low, and in these cases performance is already close to chance. 