%!Rnw root = ../Joni_Paakko_-_Thesis.Rnw

As already briefly mentioned in the introduction, in Bayesian statistics uncertainty about parameters of a statistical model is represented by a joint probability distribution over them. Also, usually \textit{prior} and \textit{posterior} probability distributions (the terms \textit{prior} and \textit{posterior} are usually used as a shorthand) are considered separately.

Broadly speaking, the prior represents the state of knowledge \textit{before data has been observed} while the posterior represents prior knowledge plus information learned from the data, or \textit{what is know after the data has been observed}, however as will be soon discussed, these concepts are related.

Figure \ref{fig:priorpost} represents this process graphically for a one-dimensional posterior. In the figure the shaded area is the prior. Dashed line represents information learned from the data. Posterior  - solid line in the figure - represents compromise between prior knowledge and information from the data.

\begin{figure}
\centering
<<fig=T,echo=F,fig.height = 3.5>>=
plot(NULL, xlim = c(0, 10), ylim = c(0, 1.0),
ylab = "Density", xlab = expression(sigma), axes = F)

axis(side = 1); axis(side = 2)

xcoords = seq(0, 10, 0.01)
ycoords_1 = dlnorm(xcoords, log(2.5), 0.75)
ycoords_2 = dlnorm(xcoords, log(6.0), 0.25)
ycoords_3 = ycoords_1 * ycoords_2

ycoords_1 = ycoords_1 / max(ycoords_1)
ycoords_2 = ycoords_2 / max(ycoords_2)
ycoords_3 = ycoords_3 / max(ycoords_3)

polygon(c(xcoords, 10), c(ycoords_1, 0), col = highlight_col, border = NA)
points(xcoords, ycoords_2, type = "l", lty = 2)
points(xcoords, ycoords_3, type = "l", lty = 1)
@
\caption{A simplified example of the basic concepts of Bayesian inference. The shaded area represents prior knowledge about the parameter $\sigma$. Dashed line is $P(Data | \theta)$ and the solid line is the posterior probability density, $P(\theta | Data)$}
\label{fig:priorpost}
\end{figure}

This updating is formalized in the Bayes' formula \citep{kruschke2015}:

\begin{equation}
P(\theta | Data) \propto P(\theta) P(Data | \theta)
\label{eq:bayesprop}
\end{equation} 

The posterior ($P(\theta | Data)$) is \textit{proportional to} ($\propto$) the prior probability of parameter values ($P(\theta)$) multiplied by the likelihood of of those parameter values ($P(Data | \theta)$).

Priors and posteriors are linked. Generally speaking this is true for all parametric Bayesian models, but I will clarify this in context of the psychophysical model(s) used in this thesis, since this is the most relevant context here.

On each trial current prior knowledge is used to select the optimal stimulus. In practice, before any data is collected, we start from an \textit{initial prior}. After the participant gives their response to the selected stimulus, the initial prior can be updated to a posterior. This posterior in turn becomes the prior for the next trial. In this way what is prior and what is posterior is to a degree a question of perspective. The only deviation to this rule is the initial prior.   

How the initial prior distribution is determined is a complex question. Usually mathematical convenience guides--in conjunction with prior knowledge--guides it's practical formulation, but some heuristics can be discussed. First of all, the prior distribution should take into account mathematical constraints, for example here the fact that $\sigma$ must always be positive, which is why I used the log-normal distribution in the example. Another aspect is that one can encode current knowledge in a meaningful way by choosing the prior appropriately. A \textit{weak} or \textit{non-informative} prior mostly acts as a way of regularizing the inferences and increasing computational stability, while a textit{strong} or \textit{informative} prior can encode specific subject-matter knowledge (for accessible discussion on priors and discussion on weak and strong priors see \citet{prior_choice_recommendations}). 

For determining priors for psychometric functions \citet{lee2018} recommends informative priors, and using prior predictive distributions--that is, distributions of psychometric functions--as a way of checking that the priors make sense and lead to psychometric functions that are realistic. One problem with this approach is that usually priors that assume independence among the parameters are used (an issue that is also discussed in \citet{prior_choice_recommendations}), but it is likely that the parameters of the psychometric functions have strong correlations between them.

For example psychometric functions with high values for $\sigma$ but low values for $\beta$ produce psychometric functions that can be unrealistically slow in reaching high performance levels, especially if values for $\kappa_{\mu}$ or $\kappa_{\sigma}$ parameters are significantly large. The issue of encoding such dependencies in the prior is left for future work, I will be using priors which are independent out of convenience, and choosing values that make sense for the parameters in isolation, which is admittedly not optimal.  

Lastly, one particularly useful feature of priors is that it's easy to expand models into having a multilevel structure through them, which will be discussed in more detail shortly.  

\subsection{Marginal distributions}

Of course, in models of any complexity prior and posterior probability densities are defined over a multidimensional space, e.g. in this thesis the dimensionalities range from 7 to 27. This means that we are not interested in, for example, probabilities for values of $\sigma$ parameter alone but probabilities for combinations of parameter values, for example, that $\sigma = 2.0$ and $\beta = 0.7$ and $\kappa_{\mu} = 0.1$ and so on for all of the parameters of the specific model.

Often it is useful to summarize multidimensional posterior density as its marginals. Most analyses done in this thesis will use the marginals of  the posterior probability densities somehow. Figure \ref{fig:marginals} demonstrates the relationship between a two-dimensional posterior probability density and its margins. The leftmost panel shows the two-dimensional distribution from above, and the two other figures show its marginals, which in this case correspond to flattening the two-dimensional bump along either axis. 

\begin{figure}
\centering
<<echo=F,fig.height = 3.0>>=

z_vals = seq(-3, 3, length.out = 100)
x = matrix(NaN, ncol = 100, nrow = 100)

for(i in 1:100){
  for(j in 1:100){
    x[i,j] = mvtnorm::dmvnorm(c(z_vals[i] / 0.5, z_vals[j]), mean = c(1, 0),
                     sigma = matrix(c(1.0, 0.75, 0.75, 1), 2, 2))
  }
}

par(mfrow = c(1,3))

contour(z_vals, z_vals, t(x), axes = F, xlab = "x", ylab = "y", drawlabels = F)
abline(h = -3:3, lty = 3, col = rgb(0, 0, 0, 0.5))
abline(v = -3:3, lty = 3, col = rgb(0, 0, 0, 0.5))
axis(side = 1); axis(side = 2)

plot(z_vals, colSums(x), type = "l", axes = F, ylab = "Density", xlab = "x", main = "Marginal distribution (x)")
axis(side = 1); axis(side = 2)

plot(z_vals, rowSums(x), type = "l", axes = F, ylab = "Density", xlab = "y", main = "Marginal distribution (y)")
axis(side = 1); axis(side = 2)

@
\caption{A two-dimensional posterior probability density and its marginals. }
\label{fig:marginals}
\end{figure}



 



